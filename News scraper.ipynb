{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f0b849",
   "metadata": {},
   "source": [
    "# News article web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4a549",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd40f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, sys, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fa683",
   "metadata": {},
   "source": [
    "## Times of India "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_of_india_urls(homepage_url,headless=True):\n",
    "    article_urls = []  # Define an empty list to store article URLs\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "    \n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the Times of India homepage\n",
    "            driver.get(homepage_url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'col_l_6'))\n",
    "            )\n",
    "\n",
    "            # Extract article URLs\n",
    "            article_divs = driver.find_elements(By.CLASS_NAME, 'col_l_6')  # Replace 'your_div_class' with the actual class name of the div tag\n",
    "            for div in article_divs:\n",
    "                try:\n",
    "                    # Find the <figure> tag within the <div> tag\n",
    "                    figure_tag = div.find_element(By.TAG_NAME, 'figure')\n",
    "                    # Find the <a> tag within the <figure> tag\n",
    "                    a_tag = figure_tag.find_element(By.TAG_NAME, 'a')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    article_url = a_tag.get_attribute('href')\n",
    "                    # Append the article URL to the list\n",
    "                    article_urls.append(article_url)\n",
    "                except Exception as e:\n",
    "                    continue  # Continue to the next iteration if extraction fails\n",
    "\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article URLs:\", e)\n",
    "    \n",
    "    return article_urls  # Return the list of article URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb39662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_of_india_article(url,headless=True):\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "        \n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the webpage\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'HNMDR'))\n",
    "            )\n",
    "\n",
    "            # Extract article information\n",
    "            title = driver.find_element(By.CLASS_NAME, 'HNMDR').find_element(By.TAG_NAME, 'span').text.strip()\n",
    "            publication_date = driver.find_element(By.CLASS_NAME, 'xf8Pm.byline').find_element(By.TAG_NAME, 'span').text.strip()\n",
    "            content = driver.find_element(By.CLASS_NAME, '_s30J.clearfix').text.strip()\n",
    "            newspaper_name = 'Times Of India'\n",
    "\n",
    "            article = {\n",
    "                'Title': title,\n",
    "                'Publication Date': publication_date,\n",
    "                'Content': content,\n",
    "                'Newspaper Name': newspaper_name\n",
    "            }\n",
    "\n",
    "            return article\n",
    "        \n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping content:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eda686",
   "metadata": {},
   "source": [
    "## Indian express "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb56da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indian_express_urls(homepage_url,headless=True):\n",
    "    article_urls = []  # Define an empty list to store article URLs\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the Times of India homepage\n",
    "            driver.get(homepage_url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'other-article '))\n",
    "            )\n",
    "\n",
    "            # Extract article URLs\n",
    "            article_divs = driver.find_elements(By.CLASS_NAME, 'other-article ')  # Replace 'your_div_class' with the actual class name of the div tag\n",
    "            for div in article_divs:\n",
    "                try:\n",
    "                    # Find the <figure> tag within the <div> tag\n",
    "                    figure_tag = div.find_element(By.CLASS_NAME, 'content-txt')\n",
    "                    # Find the <a> tag within the <figure> tag\n",
    "                    a_tag = figure_tag.find_element(By.TAG_NAME, 'h3')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    a_tag2 = a_tag.find_element(By.TAG_NAME, 'a')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    article_url = a_tag2.get_attribute('href')\n",
    "                    # Append the article URL to the list\n",
    "                    article_urls.append(article_url)\n",
    "                except Exception as e:\n",
    "                    continue  # Continue to the next iteration if extraction fails\n",
    "\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article URLs:\", e)\n",
    "    \n",
    "    return article_urls  # Return the list of article URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indian_express_article(url,headless=True):\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        try:\n",
    "            # Open the webpage\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'row'))\n",
    "            )\n",
    "\n",
    "            # Extract article information\n",
    "            title = driver.find_element(By.CLASS_NAME, 'native_story_title').text.strip()\n",
    "            publication_date = datetime.now().strftime('%B %d, %Y')\n",
    "            content = driver.find_element(By.CLASS_NAME, 'story_details').find_element(By.TAG_NAME, 'p').text.strip()\n",
    "            newspaper_name = 'Indian express'\n",
    "\n",
    "            article = {\n",
    "                'Title': title,\n",
    "                'Publication Date': publication_date,\n",
    "                'Content': content,\n",
    "                'Newspaper Name': newspaper_name\n",
    "            }\n",
    "\n",
    "            return article\n",
    "        \n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(\"Error scraping content:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46940f1d",
   "metadata": {},
   "source": [
    "## Deccan chronicle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcccb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deccan_chronicle_urls(homepage_url, headless=True):\n",
    "    article_urls = []  # Define an empty list to store article URLs\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "        # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the Deccan Chronicle homepage\n",
    "            driver.get(homepage_url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_all_elements_located((By.XPATH, '//div[contains(@class, \"col-lg-3\") and contains(@class, \"col-sm-6\") and contains(@class, \"grid-margin\") and contains(@class, \"mb-5\") and contains(@class, \"mb-sm-2\")]'))\n",
    "            )\n",
    "\n",
    "            # Extract article URLs\n",
    "            article_divs = driver.find_elements(By.XPATH, '//div[contains(@class, \"col-lg-3\") and contains(@class, \"col-sm-6\") and contains(@class, \"grid-margin\") and contains(@class, \"mb-5\") and contains(@class, \"mb-sm-2\")]')\n",
    "            for div in article_divs:\n",
    "                try:\n",
    "                    # Find the <h5> tag within the <div> tag\n",
    "                    h5_tag = div.find_element(By.XPATH, './/h5[@class=\"font-weight-bold mt-3 grid-heading\"]')\n",
    "                    # Find the <a> tag within the <h5> tag\n",
    "                    a_tag = h5_tag.find_element(By.TAG_NAME, 'a')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    article_url = a_tag.get_attribute('href')\n",
    "                    # Append the article URL to the list\n",
    "                    article_urls.append(article_url)\n",
    "                except Exception as e:\n",
    "                    continue  # Continue to the next iteration if extraction fails\n",
    "\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article URLs:\", e)\n",
    "    \n",
    "    return article_urls  # Return the list of article URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff108857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deccan_chronicle_article(url,headless=True):\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "        \n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the webpage\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'news-post-wrapper-sm'))\n",
    "            )\n",
    "\n",
    "            # Extract article information\n",
    "            title = driver.find_element(By.CSS_SELECTOR, '.news-post-wrapper-sm .text-center.article-title').text.strip()\n",
    "            publication_date = driver.find_element(By.CLASS_NAME, 'date-wrapper').find_element(By.TAG_NAME, 'span').text.strip()\n",
    "            content = driver.find_element(By.CSS_SELECTOR, '.entry-main-content.dropcap > div').text.strip()\n",
    "            newspaper_name = 'The Deccan Chronicle'\n",
    "\n",
    "            article = {\n",
    "                'Title': title,\n",
    "                'Publication Date': publication_date,\n",
    "                'Content': content,\n",
    "                'Newspaper Name': newspaper_name\n",
    "            }\n",
    "\n",
    "            return article\n",
    "        \n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping content:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0eada8",
   "metadata": {},
   "source": [
    "## The mint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_mint_urls(homepage_url,headless=True):\n",
    "    article_urls = []  # Define an empty list to store article URLs\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the Times of India homepage\n",
    "            driver.get(homepage_url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'contentBox'))\n",
    "            )\n",
    "\n",
    "            # Extract article URLs\n",
    "            article_divs = driver.find_elements(By.CLASS_NAME, 'contentBox')  # Replace 'your_div_class' with the actual class name of the div tag\n",
    "            for div in article_divs:\n",
    "                try:\n",
    "                    # Find the <figure> tag within the <div> tag\n",
    "                    figure_tag = div.find_element(By.CLASS_NAME, 'newsLinsting')\n",
    "                    # Find the <a> tag within the <figure> tag\n",
    "                    a_tag = figure_tag.find_element(By.CLASS_NAME, 'newsBlock  ')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    a_tag1 = a_tag.find_element(By.TAG_NAME, 'h3')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    a_tag2 = a_tag1.find_element(By.TAG_NAME, 'a')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    article_url = a_tag2.get_attribute('href')\n",
    "                    # Append the article URL to the list\n",
    "                    article_urls.append(article_url)\n",
    "                except Exception as e:\n",
    "                    continue  # Continue to the next iteration if extraction fails\n",
    "\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article URLs:\", e)\n",
    "    \n",
    "    return article_urls  # Return the list of article URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_mint_article(url,headless=True):\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "        \n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the webpage\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'stickyCare'))\n",
    "            )\n",
    "\n",
    "            # Extract article information\n",
    "            title = driver.find_element(By.CSS_SELECTOR, '.headline').text.strip()\n",
    "            publication_date = driver.find_element(By.CLASS_NAME, 'newTimeStamp').text.strip()\n",
    "            content = driver.find_element(By.CLASS_NAME, 'mainArea').find_element(By.TAG_NAME,'p').text.strip()\n",
    "            newspaper_name = 'The mint'\n",
    "\n",
    "            article = {\n",
    "                'Title': title,\n",
    "                'Publication Date': publication_date,\n",
    "                'Content': content,\n",
    "                'Newspaper Name': newspaper_name\n",
    "            }\n",
    "\n",
    "            return article\n",
    "        \n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping content:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888fbf2",
   "metadata": {},
   "source": [
    "## Economic times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a22075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def economic_times_urls(homepage_url, headless=True):\n",
    "    article_urls = []  # Define an empty list to store article URLs\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "        # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the Times of India homepage\n",
    "            driver.get(homepage_url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//*[@id=\"topStories\"]'))\n",
    "            )\n",
    "\n",
    "            # Find the <ul> tag within the <div> tag\n",
    "            ul_tag = driver.find_element(By.XPATH, '//*[@id=\"topStories\"]/ul')\n",
    "            # Find all <li> tags within the <ul> tag\n",
    "            li_tags = ul_tag.find_elements(By.TAG_NAME, 'li')\n",
    "            # Iterate over each <li> tag\n",
    "            for li_tag in li_tags:\n",
    "                try:\n",
    "                    # Find the <a> tag within the <li> tag\n",
    "                    a_tag = li_tag.find_element(By.TAG_NAME, 'a')\n",
    "                    # Get the value of the href attribute from the <a> tag (article URL)\n",
    "                    article_url = a_tag.get_attribute('href')\n",
    "                    # Append the article URL to the list\n",
    "                    article_urls.append(article_url)\n",
    "                except Exception as e:\n",
    "                    continue  # Continue to the next iteration if extraction fails\n",
    "\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting article URLs:\", e)\n",
    "\n",
    "    return article_urls  # Return the list of article URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddbcf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def economic_times_article(url,headless=True):\n",
    "    try:\n",
    "        # Path to the Chrome WebDriver\n",
    "        webdriver_path = '/Users/casarulez/chromedriver-mac-arm64/chromedriver'\n",
    "        \n",
    "\n",
    "       # Configure Chrome options\n",
    "        chrome_options = Options()\n",
    "        if headless:\n",
    "            chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "\n",
    "        # Configure the WebDriver with Chrome options\n",
    "        service = Service(webdriver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "        try:\n",
    "            # Open the webpage\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the article elements to be visible\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, 'article_wrap'))\n",
    "            )\n",
    "\n",
    "            # Extract article information\n",
    "            title = driver.find_element(By.CSS_SELECTOR, '.artTitle.font_faus').text.strip()\n",
    "            publication_date = driver.find_element(By.CLASS_NAME, 'jsdtTime').text.strip()\n",
    "            content = driver.find_element(By.XPATH, '/html/body/main/div[11]/div/div[1]/div[3]/div/article/div[2]').text.strip()\n",
    "            newspaper_name = 'The economic times'\n",
    "\n",
    "            article = {\n",
    "                'Title': title,\n",
    "                'Publication Date': publication_date,\n",
    "                'Content': content,\n",
    "                'Newspaper Name': newspaper_name\n",
    "            }\n",
    "\n",
    "            return article\n",
    "        \n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping content:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af9546",
   "metadata": {},
   "source": [
    "## Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86381a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the filename for the CSV file\n",
    "csv_filename = 'extracted_articles.csv'\n",
    "\n",
    "times_of_india_extracted_urls = times_of_india_urls('https://timesofindia.indiatimes.com/', headless=True)\n",
    "deccan_chronicle_extracted_urls=deccan_chronicle_urls('https://www.deccanchronicle.com/', headless=True)\n",
    "indian_express_extracted_urls = indian_express_urls('https://indianexpress.com/', headless=True)\n",
    "the_mint_extracted_urls = the_mint_urls('https://www.livemint.com/', headless=True)\n",
    "economic_times_extracted_urls = economic_times_urls('https://economictimes.indiatimes.com/', headless=True)\n",
    "\n",
    "\n",
    "extracted_articles = []\n",
    "\n",
    "\n",
    "for url in times_of_india_extracted_urls:\n",
    "    article_data = times_of_india_article(url, headless=True)\n",
    "    if article_data:\n",
    "        extracted_articles.append(article_data)\n",
    "\n",
    "for url in indian_express_extracted_urls:\n",
    "    article_data = indian_express_article(url, headless=True)\n",
    "    if article_data:\n",
    "        extracted_articles.append(article_data)\n",
    "\n",
    "\n",
    "for url in deccan_chronicle_extracted_urls:\n",
    "    article_data = deccan_chronicle_article(url, headless=True)\n",
    "    if article_data:\n",
    "        extracted_articles.append(article_data)\n",
    "        \n",
    "for url in the_mint_extracted_urls:\n",
    "    article_data = the_mint_article(url, headless=True)\n",
    "    if article_data:\n",
    "        extracted_articles.append(article_data)\n",
    "        \n",
    "for url in economic_times_extracted_urls:\n",
    "    article_data = economic_times_article(url, headless=True)\n",
    "    if article_data:\n",
    "        extracted_articles.append(article_data)\n",
    "        \n",
    "        \n",
    "if extracted_articles:\n",
    "    df = pd.DataFrame(extracted_articles)\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if os.path.exists(csv_filename):\n",
    "        # Append to existing CSV file\n",
    "        df.to_csv(csv_filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Create new CSV file\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "    print(\"Data appended to CSV file:\", csv_filename)\n",
    "else:\n",
    "    print(\"Error: Unable to scrape any articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e179dc",
   "metadata": {},
   "source": [
    "## Testing URL extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4970863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_test=economic_times_urls('https://economictimes.indiatimes.com/', headless=False)\n",
    "print(url_test)\n",
    "print(len(url_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d496c",
   "metadata": {},
   "source": [
    "## Testing article extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92982c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_articles_test=[]\n",
    "for url in range(0,3):\n",
    "    article_data = deccan_times_article(url_test[url], headless=False)\n",
    "    if article_data:\n",
    "        extracted_articles_test.append(article_data)\n",
    "        df_test = pd.DataFrame(extracted_articles_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02233aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_articles_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6773bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
